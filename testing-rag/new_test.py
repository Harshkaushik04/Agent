import llama_cpp
print(llama_cpp.llama_supports_gpu_offload())
